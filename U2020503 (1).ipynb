{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAuxFuAGWo49"
   },
   "source": [
    "![GIKI_logo_with_text.png](attachment:GIKI_logo_with_text.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRERiJDFWo4_"
   },
   "source": [
    "<h1><center>Deep Neural Networks (AI341) - Assignment No. 2 </center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P6IQKxIMWo5A"
   },
   "source": [
    "# Classification of the CIFAR-10 dataset\n",
    "\n",
    "The [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) provides 60000 32x32-pixel images, classified into 10 categories.  The figure below provides a random sample of some images in each category.\n",
    "\n",
    "![images.png](Dataset.png)\n",
    "\n",
    "In this assignment, you will learn how to build a Convolutional Neural Network (CNN), which (when trained) will be able to automatically classify new images into one of these categories, you will also learn to optimize your model through different techniques covered in your class.  We will make use of the [Keras library](https://www.tensorflow.org/guide/keras) which provides a high-level interface to TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfUL-yXYWo5A"
   },
   "source": [
    "# Table of content\n",
    "\n",
    "[1. Introduction to keras](#intro_keras)<br>\n",
    "[2. A first look at the data set](#dataset)<br>\n",
    "[3. A first naive model](#first_model)<br>\n",
    "[4. Interpreting the results](#results)<br>\n",
    "[4.1 Making predictions](#results_prediction)<br>\n",
    "[4.2 Evaluating the results](#results_evaluation)<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mhzfAmIWo5B"
   },
   "source": [
    "<a id='intro_keras'></a>\n",
    "\n",
    "## 0 - Introduction to Keras\n",
    "\n",
    "Keras is a high-level API to build and train deep learning models. It's used for fast prototyping, advanced research, and production, with three key advantages:\n",
    "\n",
    "- __User friendly__: Keras has a simple, consistent interface optimized for common use cases. It provides clear and actionable feedback for user errors.\n",
    "- __Modular and composable__: Keras models are made by connecting configurable building blocks together, with few restrictions.\n",
    "- __Easy to extend__: Write custom building blocks to express new ideas for research. Create new layers, loss functions, and develop state-of-the-art models.\n",
    "\n",
    "In Keras, models are built by assymbling multiple layers.  Suppose we want to create a new multilayer perceptron model to categorize 128-feature data into 10 labeled categories.  Keras code looks like:\n",
    "\n",
    "```python\n",
    "# Create a sequential model\n",
    "model = keras.models.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model\n",
    "model.add(layers.Dense(64, activation='relu'), input_shape=[128])\n",
    "# Add another\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "# Add a softmax layer with 10 output units\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "```\n",
    "\n",
    "The `input_shape` argument must be given for the first layer in the model, however all other layers will automatically determine the input shape based on the previous layer in the model.  Note that the code above is substantially simpler than the corresponding TensorFlow code.  This is particularly useful for building convolutional or other types of layers, as we will see.\n",
    "\n",
    "Once built, a model's learning can be configured with the `compile()` function:\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy', \n",
    "    optimizer=tf.train.AdamOptimizer(0.001), \n",
    "    metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "In this case, a cross-entropy loss function is used with the ADAM optimization algorithm.  The `metrics` argument allows the model to keep track of a number of [training metrics](https://www.tensorflow.org/api_docs/python/tf/keras/metrics) during training.\n",
    "\n",
    "Once configured, training is performed using the `fit()` function.\n",
    "\n",
    "```python\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "```\n",
    "\n",
    "The function takes an array-like (could be numpy array) of data and the corresponding target values, and performs the optimization of the learnable parameters in the model.  See the documentation for the [fit()](https://www.tensorflow.org/api_docs/python/tf/keras/models/Model#fit) function for more details.\n",
    "\n",
    "Once trained, the model can be used to predict, using the `predict()` function. \n",
    "\n",
    "```python\n",
    "prediction = model.predict(new_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rISjjh49Wo5C"
   },
   "source": [
    "<a id='dataset'></a>\n",
    "# 1 - Understanding the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4cUDkxjWo5C"
   },
   "source": [
    "Begin by importing the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOMzz8qqWo5C",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.layers import Dense, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Dropout, BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0QQRB_imWo5D"
   },
   "source": [
    "Understanding your dataset is the first prerequisit to training any model.  The CIFAR-10 dataset can be loaded directly from Keras.\n",
    "\n",
    "**1) Download the dataset. See [`keras.datasets`](https://keras.io/datasets/) for how to download the data, and in what format it is provided.  Note that the dataset is already divided into a training set of 50000 images, and a test set of 10000.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SVUjNToaWo5D"
   },
   "outputs": [],
   "source": [
    "\n",
    "(X_train,Y_train),(X_test,Y_test)= tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GmwSArOLWo5E"
   },
   "source": [
    "**2) Verify that the shape of the image and target arrays are what you expect.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lvpwq64OWo5E"
   },
   "outputs": [],
   "source": [
    "print(\"X_train {} and Y_train{}\".format(X_train.shape,Y_train.shape))\n",
    "print(\"X_test {} and Y_test{}\".format(X_test.shape,Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2p_BeGgLWo5E"
   },
   "source": [
    "We now create a list of labels corresponding to the 10 categories.  It will be used to convert the 0-9 digits in the target arrays to string labels. The categories are labeled as follows:\n",
    "\n",
    "  0. airplane\n",
    "  1. automobile\n",
    "  2. bird\n",
    "  3. cat\n",
    "  4. deer\n",
    "  5. dog\n",
    "  6. frog\n",
    "  7. horse\n",
    "  8. ship\n",
    "  9. truck\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StqNpxfqWo5E"
   },
   "outputs": [],
   "source": [
    "Classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz_l2BP9Wo5F"
   },
   "source": [
    "**3) Normalize the image data from [0,255] to be [0,1].  Normalizing improves model training (to test this, you can comment out the normalization later).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7RUdrbAWo5F"
   },
   "outputs": [],
   "source": [
    "X_train=X_train/255\n",
    "X_test=X_test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aDJlGPiaWo5G"
   },
   "source": [
    "**4) Convert the target arrays to one-hot encodings.  Hint: checkout the [`keras.utils.np_utils.to_categorical()`](https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nnoFuGvWo5G"
   },
   "outputs": [],
   "source": [
    "Y_train=to_categorical(Y_train,num_classes=10)\n",
    "Y_test=to_categorical(Y_test,num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QDultGDkWo5G"
   },
   "source": [
    "**5) Visualize some images in each category using the `imshow()` function in `matplotlib.pyplot`.  Can you recreate the figure below?  Hint: the below figure was created using the first 8 images belonging to each category in the training data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lk_NzBtZWo5H"
   },
   "source": [
    "![Dataset.png](attachment:Dataset.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_JfO_oXWo5H"
   },
   "outputs": [],
   "source": [
    "Classes=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qiCHrlD_YPx2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGO8ZQpDWo5H"
   },
   "source": [
    "<a id='first_model'></a>\n",
    "\n",
    "# 2 - First naive model\n",
    "\n",
    "In order to better understand the importance of CNNs, it is instructive to first see how well a naive dense network performs on the dataset.\n",
    "\n",
    "**6) Create a sequential model with 4 `Dense` hidden layers of 2048, 1024, 512, and 256 nodes each, with ReLU activation, and a final output layer of 10 nodes. Compile the model with a `categorical_crossentropy` loss, using the SGD optimizer, and the `accuracy` metric. \n",
    "Note that you will need to use the `Flatten` layer first in order to convert the 3D (x, y, rgb) image data into 1D.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fygY5Jt3Wo5I"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.python import metrics\n",
    "from tensorflow.keras import models,layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "model=models.Sequential()\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2048,activation='relu'))\n",
    "model.add(layers.Dense(1024,activation='relu'))\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(10))\n",
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKQMqAEWWo5I"
   },
   "source": [
    "**7) Compute by hand the total number of trainable parameters (weights and biases) in the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6DUg0w9G6p5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkU2QYOUWo5I"
   },
   "source": [
    "In the first layer we have 3072 neurons which is fully connected to hidden layer  2048  which results in 3072x2048+2048 =6293504\n",
    "In the second layer we have 2048 neurons which is connected with dense layer having neurons 1024 which result in 2048X1024+1024=2098176\n",
    "and so on for all till end \n",
    "for end we have 256 neurons in hidden layer which connected with output layer \n",
    " having 10 neurons which result in 256X10+10=2570\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a39gHk_AWo5I"
   },
   "source": [
    "**8) Use the `summary()` function on model to get a text summary of the model.  Did you compute the number of parameters correctly?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvNXYl4WWo5I"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "86LP0a_oWo5J"
   },
   "source": [
    "**9) Train the model:**\n",
    "  - Start with a small batch size of 32 and train for 10 epochs\n",
    "  - Use early stopping on the validation accuracy with a patience of 2 (use 10% of your training set as the validation set)\n",
    "  \n",
    "**How does the model perform?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3gspprBPWo5J"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs=10,batch_size=32,callbacks=EarlyStopping(patience=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I3M8J6LLWo5J"
   },
   "source": [
    "**10) Try changing the batch size to see if there is any improvement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q1dzKcGwWo5J"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train,Y_train,epochs=10,batch_size=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE1mW7ZCWo5K"
   },
   "source": [
    "**11) Try adding batch normalization after each hidden layer.  Any better?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_VsyQnWWo5K"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python import metrics\n",
    "from tensorflow.keras import models,layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "model=models.Sequential()\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(2048,activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.99, epsilon=0.002))\n",
    "model.add(layers.Dense(1024,activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=1, epsilon=0.003))\n",
    "model.add(layers.Dense(512,activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.923, epsilon=0.003))\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.BatchNormalization(momentum=0.94, epsilon=0.034))\n",
    "model.add(layers.Dense(10))\n",
    "model.compile(optimizer='SGD',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n",
    "model.fit(X_train,Y_train,epochs=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qKzWnIUSWo5K"
   },
   "source": [
    "<a id='cnn'></a>\n",
    "\n",
    "# 3 - Convolutional Neural Network\n",
    " \n",
    "\n",
    "Convolutional neural networks allow us to do drastically better on this dataset (and many image classification problems in general). In this task, you will build a convolutional network and see how it performs during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8BPgo0NdWo5K"
   },
   "source": [
    "**12) Create a new model with the following layers**\n",
    "  - 3x3 2D convolution with zero padding (same), 32 filters\n",
    "  - ReLU activation\n",
    "  - 3,3 2D convolution, no padding, 32 filters\n",
    "  - ReLU activation\n",
    "  - Max pooling with size (2,2)\n",
    "  - 3x3 2D convolution, no padding, 64 filters\n",
    "  - ReLU activation\n",
    "  - 3x3 2D convolution, no padding, 64 filters\n",
    "  - ReLU activation\n",
    "  - Max pooling with size (2,2)\n",
    "  - Flatten\n",
    "  - Dense layer with 512 nodes, ReLU activation\n",
    "  - Softmax output layer with 10 nodes\n",
    "  \n",
    "**Compile the network with same optimizer and metrics as the dense network.**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5thpvAqwWo5K"
   },
   "outputs": [],
   "source": [
    "from keras.layers.attention.multi_head_attention import activation\n",
    "from tensorflow import keras\n",
    "model1=models.Sequential()\n",
    "model1.add(keras.layers.ZeroPadding2D(padding=(2, 2)))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(512,activation='relu'))\n",
    "model1.add(layers.Dense(10,activation='Softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n",
    "model1.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVQW4ZymWo5K"
   },
   "source": [
    "**13) Compute by hand the number of trainable parameters in this network.  Are there more or less than the more simple dense network?  Why?  Confirm with `summary()`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkaOk_gYWo5K"
   },
   "outputs": [],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKAVf7NWWo5L"
   },
   "source": [
    "**14) Use the same training procedure as before for 10 epochs and batch size of 32. How does the validation accuracy change with each epoch?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iNq__EMyWo5L"
   },
   "outputs": [],
   "source": [
    "(X_train,Y_train),(X_test,Y_test)= tf.keras.datasets.cifar10.load_data()\n",
    "X_test=X_test/255\n",
    "Y_test=Y_test/255\n",
    "Y_train=to_categorical(Y_train,num_classes=10)\n",
    "Y_test=to_categorical(Y_test,num_classes=10)\n",
    "model1.fit(X_train,Y_train,batch_size=32,epochs=10,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K3l9QuTWo5L"
   },
   "source": [
    "**15) Increase the batch size to 64 and retrain.  Better or worse?  Try 128 as well.  How does increasing the batch size improve the training?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CLCBeILwWo5L"
   },
   "outputs": [],
   "source": [
    "#With batch size of 64 accuracy is improving \n",
    "model1.fit(X_train,Y_train,epochs=10,batch_size=64)\n",
    "#With batch size of 128 accuracy is improving \n",
    "model.fit(X_train,Y_train,epochs=10,batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1Y4kD5aWo5L"
   },
   "source": [
    "**16) Note how the validation accuracy begins to decrease at some point, while the training accuracy continues to increase.  What is this phenomena called?  Try adding 3 dropout layers to the model, one before each max pooling layer and one before the last layer, using a dropout ratio of 0.25.  Does this improve the model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPqWRiz3Wo5L"
   },
   "outputs": [],
   "source": [
    "model1=models.Sequential()\n",
    "model1.add(keras.layers.ZeroPadding2D(padding=(2, 2)))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(512,activation='relu'))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.Dense(10,activation='Softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "auTXRYTmWo5L"
   },
   "source": [
    "**17) Play with batch normalization.  For example, add batch normalization layers after each dropout layer.  Do you notice a faster increase in the model improvement? Why?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRPhVo8MWo5L"
   },
   "outputs": [],
   "source": [
    "model1=models.Sequential()\n",
    "model1.add(keras.layers.ZeroPadding2D(padding=(2, 2)))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu',padding='valid'))\n",
    "model1.add(layers.Conv2D(32,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.BatchNormalization(momentum=1, epsilon=0.003))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
    "model1.add(layers.MaxPool2D(pool_size=(2, 2)))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.BatchNormalization(momentum=1, epsilon=0.003))\n",
    "model1.add(layers.Flatten())\n",
    "model1.add(layers.Dense(512,activation='relu'))\n",
    "model1.add(layers.Dropout(0.25))\n",
    "model1.add(layers.BatchNormalization(momentum=1, epsilon=0.003))\n",
    "model1.add(layers.Dense(10,activation='Softmax'))\n",
    "model1.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "               metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdnKR0xYWo5L"
   },
   "source": [
    "<a id='results'></a>\n",
    "\n",
    "# 4 - Interpreting the results\n",
    " \n",
    "<a id='results_prediction'></a>\n",
    "\n",
    "## 4.1 - Making predictions\n",
    "\n",
    "Assuming all went well during the previous tasks, you can now predict the category of a new image!  Here are a few examples of my predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VgpZgUszWo5M"
   },
   "source": [
    "![Results.png](attachment:Results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elFZ6hM4Wo5M"
   },
   "source": [
    "**18) Use `predict` on your trained model to test its prediction on a few example images of the test set. Using `imshow` and `hbar` from `matplotlib.pyplot`, try to recreate the image above for a few example images.**\n",
    "\n",
    "<!---**Hint:** at this point, it is probably convenient to use the `save` and `load_model` functions from Keras.  You can save the model after training it, and then decide to load from saved file instead of building a new one (if available) on successive runs.--->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctDcEqLVWo5M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IWB6sJYxWo5M"
   },
   "source": [
    "<a id='results_evaluation'></a>\n",
    "\n",
    "## 4.2 Evaluating the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNtzZj4EWo5M"
   },
   "source": [
    "A confusion matrix is often used in supervised learning to understand how well (or not) each category is being classified.  Each element (i,j) in the confusion matrix represents the predicted class j for each true class i.  Consider the following 10 predictions for a 2 category model predicting male or female:\n",
    "\n",
    "| example     | true category  | predicted category  |\n",
    "|-------------|----------------|---------------------|\n",
    "| 1           | male           | male                |\n",
    "| 2           | female         | male                |\n",
    "| 3           | female         | female              |\n",
    "| 4           | male           | male                |\n",
    "| 5           | male           | female              |\n",
    "| 6           | male           | male                |\n",
    "| 7           | female         | female              |\n",
    "| 8           | male           | female              |\n",
    "| 9           | female         | female              |\n",
    "| 10          | female         | female              |\n",
    "\n",
    "Based on the above data, the model is accurate 70% of the time.  The confusion matrix is\n",
    "\n",
    "|        | male | female |\n",
    "|--------|------|--------|\n",
    "| male   | 3    | 2      |\n",
    "| female | 1    | 4      |\n",
    "\n",
    "The confusion matrix gives us more information than a simple accuracy measurement.  In this case, we see that the class female has a higher accuracy over male.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtD7JU1BWo5M"
   },
   "source": [
    "**19) Create the confusion matrix for the CIFAR-10 dataset using the test data.  What does it tell you about the relationships between each class?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ik56oqPpWo5M",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_EKHhllWo5M"
   },
   "source": [
    "<a id='pretrained_cnn'></a>\n",
    "# 5 - Improving on current performances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lhfDidRJWo5M"
   },
   "source": [
    "**20) Play with different CNN architectures. Provide a few attempts (atleast 1 and atmost 3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KnAFaRkEWo5M"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIJ0JMRPWo5M"
   },
   "source": [
    "Note that several pre-trained networks are directly accessible via keras (see https://nbviewer.jupyter.org/github/fchollet/deep-learning-with-python-notebooks/blob/master/5.3-using-a-pretrained-convnet.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "anaconda-cloud": {},
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
